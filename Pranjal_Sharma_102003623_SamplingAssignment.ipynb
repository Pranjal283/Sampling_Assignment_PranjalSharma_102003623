{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "g2wBurR1_GhC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from scipy.stats import norm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imblearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1NbC5w5BNO2",
        "outputId": "00ece93b-6b11-4ba2-9769-ffcb8b399041"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.8/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.8/dist-packages (from imblearn) (0.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data=pd.read_csv(\"Creditcard_data.csv\")"
      ],
      "metadata": {
        "id": "gaXLGidZBR1M"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.Class.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfoG-bJiBitf",
        "outputId": "534e54ec-9d68-4e63-890b-de15f1e9d3c4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    763\n",
              "1      9\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop('Class',axis=1)\n",
        "Y = data['Class']"
      ],
      "metadata": {
        "id": "HQZL5yFyB4BC"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NYtB6uECIK7",
        "outputId": "8ec94974-bf59-48d0-bb5b-a95690236b76"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(772, 30)\n",
            "(772,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn import over_sampling\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "rus = RandomOverSampler(random_state=0)\n",
        "X_train_os, Y_train_os = rus.fit_resample(X,Y)"
      ],
      "metadata": {
        "id": "IGWcx-ybCXhV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before Over Sampling :\" , Counter(Y))\n",
        "print(\"After Over Sampling :\" , Counter(Y_train_os))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOp7QjRXDo9_",
        "outputId": "b35428cc-15b9-493d-b664-41c1b55d0afd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Over Sampling : Counter({0: 763, 1: 9})\n",
            "After Over Sampling : Counter({0: 763, 1: 763})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df = pd.concat([X_train_os, Y_train_os], axis=1)\n",
        "balanced_df.to_csv('balanced_dataset.csv', index=False)\n",
        "print(\"Balanced Dataset Created...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0TUrQbuDs5u",
        "outputId": "3e928280-b5c9-4aac-901b-4755372d2f38"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced Dataset Created...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculation of sample size using sample size detection formula\n",
        "p = np.sum(Y_train_os) / len(Y_train_os)\n",
        "\n",
        "# Set the confidence level and margin of error\n",
        "confidence_level = input(\"Enter the Confidence Level(in %) :\")  \n",
        "confidence_level = float(confidence_level)/100\n",
        "\n",
        "alpha = 1-confidence_level\n",
        "print(\"The Margin of Error :\",alpha)\n",
        "\n",
        "z_score = norm.ppf(1-alpha/2)\n",
        "print(\"Z-Score is :\",z_score)\n",
        "\n",
        "n = int(np.ceil((z_score**2 * p * (1-p)) / (alpha**2)))\n",
        "print(\"Sample Size is :\",n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kaLBuTxDwSZ",
        "outputId": "b125db03-c492-4fd4-b6bb-f597467be1b3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the Confidence Level(in %) :95\n",
            "The Margin of Error : 0.050000000000000044\n",
            "Z-Score is : 1.959963984540054\n",
            "Sample Size is : 385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_datasets = []"
      ],
      "metadata": {
        "id": "6HGergVED6Uy"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple random sampling\n",
        "sample0 = balanced_df.sample(n, replace=False)\n",
        "sample_datasets.append(sample0)"
      ],
      "metadata": {
        "id": "MNhEwKtTD9os"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#systematic Sampling\n",
        "sampling_interval = int(len(balanced_df) / n) # Sample every 10th row\n",
        "\n",
        "# Create a list of indices to sample\n",
        "indices = np.arange(start=0, stop=len(balanced_df), step=sampling_interval)[:n]\n",
        "\n",
        "# Sample the dataset using the indices\n",
        "sample1 = balanced_df.iloc[indices]\n",
        "sample_datasets.append(sample1)\n"
      ],
      "metadata": {
        "id": "t8_7FlrtEQ4g"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster Sampling\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Separate the data into two clusters based on the class column\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(balanced_df.iloc[:, :-1])\n",
        "clusters = kmeans.predict(balanced_df.iloc[:, :-1])\n",
        "balanced_df['cluster'] = clusters\n",
        "\n",
        "# Calculate the proportion of each cluster in the data\n",
        "proportions = balanced_df['cluster'].value_counts(normalize=True)\n",
        "\n",
        "# Set the desired sample size and calculate the number of samples to take from each cluster\n",
        "desired_sample_size = n\n",
        "sample_sizes = np.round(proportions * n).astype(int)\n",
        "\n",
        "# Initialize an empty list to store the sampled data\n",
        "sample2 = []\n",
        "\n",
        "# Iterate over each cluster and take a random sample of the appropriate size\n",
        "for cluster, size in sample_sizes.iteritems():\n",
        "    cluster_data = balanced_df[balanced_df['cluster'] == cluster]\n",
        "    sample = cluster_data.sample(n=size, random_state=0)\n",
        "    sample2.append(sample)\n",
        "\n",
        "# Concatenate the sampled data into a single DataFrame\n",
        "sample2 = pd.concat(sample2)\n",
        "\n",
        "# Remove the cluster column from the sampled data\n",
        "sample2 = sample2.drop('cluster', axis=1)\n",
        "sample_datasets.append(sample2)"
      ],
      "metadata": {
        "id": "uuSioqLeFWIQ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified Sampling\n",
        "sample3=balanced_df.groupby('Class',group_keys=False).apply(lambda x: x.sample(frac=.2523))\n",
        "sample_datasets.append(sample3)"
      ],
      "metadata": {
        "id": "tzspFuYeFqVN"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convenience Sampling\n",
        "# Separate the 0 and 1 class observations into two separate dataframes and Conducting convenience sampling by selecting n/2 observations from each class\n",
        "zeros_df = balanced_df[balanced_df['Class'] == 0].sample(int(n/2), random_state=1)\n",
        "ones_df = balanced_df[balanced_df['Class'] == 1].sample(int(n/2), random_state=1)\n",
        "\n",
        "# Combine the sampled dataframes into a new balanced dataset\n",
        "sample4 = pd.concat([zeros_df, ones_df])\n",
        "sample_datasets.append(sample4)"
      ],
      "metadata": {
        "id": "CWwXpIv9FwLG"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sample in enumerate(sample_datasets):\n",
        "    sample.to_csv(f'sample_dataset_{i}.csv', index=False)\n",
        "     "
      ],
      "metadata": {
        "id": "15GDEIHLF8k3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    Pipeline([('scaler', StandardScaler()),('lr', LogisticRegression(max_iter=1000))]),\n",
        "    KNeighborsClassifier(),GaussianNB(),RandomForestClassifier(), SVC()]\n",
        "\n",
        "# Define a list of model names for the table\n",
        "model_names = ['Logistic Regression', 'KNN', 'Naive bayes', 'Random forest', 'SVC']\n",
        "\n",
        "# Define a table to store the results\n",
        "results_table = pd.DataFrame(columns=['Dataset', *model_names])\n",
        "\n",
        "# Loop over each sample dataset and each model to compute accuracy\n",
        "for i, sample in enumerate(sample_datasets):\n",
        "    X = sample.iloc[:, :-1]\n",
        "    y = sample.iloc[:, -1]\n",
        "    row = {'Dataset': f'Sampling {i+1}'}\n",
        "    for j, model in enumerate(models):\n",
        "        model.fit(X, y)\n",
        "        y_pred = model.predict(X)\n",
        "        accuracy = accuracy_score(y, y_pred)\n",
        "        row[model_names[j]] = f'{accuracy:.3f}'\n",
        "    results_table = results_table.append(row, ignore_index=True)\n",
        "\n",
        "# Transpose the table so that the model names are in the first column and dataset names are in the top row\n",
        "results_table = results_table.set_index('Dataset').T.rename_axis('Model', axis=0)\n",
        "\n",
        "# Print the results table\n",
        "print(results_table)\n",
        "results_table.to_csv(f'Final_Solution.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgHZkOATGAAU",
        "outputId": "e6ce15c9-6865-42e4-b2ed-fd1318f47319"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset             Sampling 1 Sampling 2 Sampling 3 Sampling 4 Sampling 5\n",
            "Model                                                                     \n",
            "Logistic Regression      0.932      0.878      0.935      0.997      0.997\n",
            "KNN                      0.956      0.953      0.964      0.992      0.997\n",
            "Naive bayes              0.868      0.735      0.644      0.915      0.911\n",
            "Random forest            1.000      1.000      1.000      1.000      1.000\n",
            "SVC                      0.743      0.774      0.683      0.995      0.992\n"
          ]
        }
      ]
    }
  ]
}